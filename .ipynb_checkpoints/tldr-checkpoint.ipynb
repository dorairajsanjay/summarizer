{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tl;dr \n",
    "\n",
    "### A text summarizer\n",
    "\n",
    "### Sanjay Dorairaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The title of this paper is tl;dr (too long don't read). The goal of this work is to create a neural network based multi-line text summarizer building on state of the art methods that exist today. The final goal of this work is to incorporate this summarizer into various applications in need of this feature, such as, websites (via a Chrome, Firefox plugin or similar) and in blog sites and wikis. \n",
    "\n",
    "The internet and other catalysts such as big data have set the stage for the exponential growth of available information. This information often manifests itself in news articles, blogs, research papers, social media conversations and so on. Consuming and keeping track of all this information is extremely difficult. While on the one hand, information provides a definite edge and opens doors to new opportunities, navigating and keeping track of the flood of information is impossible since our single-processor minds lack the capacity to process all this information. \n",
    "\n",
    "The parallel processing capability of computers (read GPUs) and the advances in neural network technologies however, can help us get closer to our goal of fishing meaningful information i.e. information that matters to us, from the vast ocean of available information. \n",
    "\n",
    "For a long time, the primary method of delivering meaningful information to users has been through information retrieval and recommendation models. Take for example, Google's Page Rank algorithm [1]. This algorithm ranks all pages on the internet in order of quality based on how many sites recommend (or point to) a site. Other recommendation models perform custom recommendations, where information content is matched based on other users with similar tastes or similar articles consumed in the past. While these approaches help reduce the amount of information directed at its consumer, the filtered information is itself sufficiently large for any one individual to process. Text summarizers attempt to deal with this second issue. By compressing text into byte-sized pieces, they help minimize consumption time bringing us even closer to our goal of being \"up to date\".\n",
    "\n",
    "This paper examines some of the latest advances in text summarization and attempts to build upon past work to create a relatively more effective text summarizer. The keys ideas explored in this paper are the RNN/LSTM based sequence-to-sequence network, attention, pointer-generation, beam search and the noisy channel model. \n",
    "\n",
    "<< FINDINGS >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here, we introduce various techniques of text summarization that are available today and key definitions and nomenclature surrounding these techniques. We also talk about the key ideas used in this paper and how these ideas differ from existing research. \n",
    "\n",
    "There are two popular forms of text summarization - extractive and abstractive [2] . Extractive text summmarization seeks to reproduce parts of the input text in the output text summary. On the other hand, abstractive text summarization takes a more creative approach. It attempts to summarize by creating an original sequence of words that attempt to capture the gist of the input text. This method has been traditionally difficult to implement since it requires a fundamental understanding of creative sentence construction which has eluded us for a long time. However, with the recent advances in neural networks, especially in the area of recursive neural networks or RNNs, we appear to be closer to our goal of creative sentence construction. \n",
    "\n",
    "Although a vanilla sequence to sequence (or encoder/decoder) networks give us a good start they have the following issues (1) they inaccurately reproduce factual details, (2) have issues dealing with out of vocabulary words (OOV) and 3) repeat themselves (sic [2]). The beam search algorithm is used to help pick the best complete output sentence by conditioning each decoder output on previously emitted words. Attention models [3] help deal with the first issue, while pointer generation models help deal with the second issue. [2] suggests using a coverage model to deal with the issue of repeating words. In this paper we take a slightly different approach. The last beam search decision or prioritization is conditioned upon the probability of the output being grammatically correct. The POS validator is an independently generated classifer model.\n",
    "\n",
    "This paper pulls together the latest ideas in sequence to sequence networks, attention, beam search and pointer-generation networks. It enhances this framework with a parts of speech based (POS) noisy channel model for optimizing beam search performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In this section we explain each of the technologies that are used in this paper. \n",
    "\n",
    "## Sequence to sequence models\n",
    "\n",
    "A sequence to sequence network [4] is also referred to an encoder-decoder model. This model allows us to generate a sequence of output corresponding to a given input sequence. The model typically uses an enhanced RNN cell, such as a bi-LSTM, multi-stacked model, in order to process input text. The bi-LSTM allows the model to leverage both the future and the past to make predictions and the LSTMs address the issue of vanishing or exploding gradients. This part of the processing happens in the encoder network. The last hidden state of the encoder network is expected to hold knowledge of the entire input sentence. This is fed into another LSTM network, known as the decoder network. The decoder network takes as input the last hidden state from the encoder network and the previous output word and generates the next word. The decoder stops generating words when it generates a special word that denotes the end of the output sequence.\n",
    "\n",
    "![Seq2seq Network](vanilla-seq2seq-network.png)\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "** Figure 1: Vanilla Sequence to Sequence Network **\n",
    "</center>\n",
    "\n",
    "Key considerations when designing a sequence to sequence network during text summarization is to ensure that each summary is to ensure that during training each summary is terminated with a special end of sequence of token. During inference, we stop sentence generation when we see this end of sequence token. Also, we use a dual-stacked bidirectional LSTM for our base model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention models\n",
    "\n",
    "The problem with vanilla sequence to sequence networks in the contex of text summarization is that they have been shown to generate inaccurate details of the undelying input text [2]. The reason for this has been largely attributed to the model's inability to compress all information about the piece of input text into a single hidden vector i.e. even LSTMs tend to forget critical information if they need to process large amount of text. \n",
    "\n",
    "Attention-based models attempt to fix this issue by allowing the decoder to glimpse the details of encoder processing and factor this information into output generation. This is analogous to how humans summarize information when looking at large volumes of text. Rather than trying to read the entire text and summarizing, we summarize a block of text and move on to summarizing the next contiguous block and so on. Attention-based models attempt to keep the decoder informed of the relative priority of words in the input text, so that the decoder can focus on relevant parts of the input text during the summarization process. \n",
    "\n",
    "![Seq2Seq Network with Attention](seq2seq-attn.png)\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "** Figure 2: Sequence to Sequence Network with Attention **\n",
    "<br>\n",
    "    <b>Source:</b> Get To the Point: Summarization with Pointer-Generator [2]\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual implementation of attention is relatively straightforward. The process of using attention is depicted realy well in the picture below extracted from Chris Dyer's slides on Modeling with Attention. For each forward and backward pass of a bi-directional LSTM, hidden state information is concatenated to create a column in an MXN matrix F, where M is the size of the hidden dimension and N is the number of words in the input sentence. A weight matrix, V, is then learned to compute an expected input embedding \\$r_t for each previous decoder input $s_{t-1}$.\n",
    "\n",
    "$r_{t} = Vs_{t-1}$\n",
    "\n",
    "An attention vector for each decoder state \\$s{t-1} using the equation below\n",
    "\n",
    "$e_t = F^Tr_{t}$,\n",
    "\n",
    "$e_t$ is known as the attention energy. It is exponentiated and normalized and then multipled with our matrix F, in order to yield a context vector $C_{t}$\n",
    "\n",
    "This context vector is fed along with $s_{t-1}$ as input into the next step of the decoder allowing the decoder to catch glimpses of the encoder hidden states and use this information to inform the decoding process.\n",
    "\n",
    "<img src=\"chrisdyer_attention_cmu.png\" alt=\"Seq2Seq Network with Attention\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "** Figure 3: Inferring Attention in a Sequence to Sequence Network **\n",
    "\n",
    "</center>\n",
    "Source: Lecture 8 - Generation Language with Attention, Chris Dyer [5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointer Generation\n",
    "\n",
    "A Pointer generation model allows for the model to handle out of vocabulary (OOV) words. At each timestep, the model chooses between whether to use abstractive output from the decoder or whether to pull information from the pointer generation mechanism. More details about this approach can be found in [2]\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search with Noisy Channel model\n",
    "\n",
    "The Beam Search algorithm allows us to generate output text that has a higher likelihood of occuring given the input text. It does this by conditioning output words during each step of the decoding process on the input text and picking partial text that has a higher likelihood of occuring for a given input sentence. Beam search tries different combinations of partial output sentences, each time picking a set of partial output sentences with the highest likelihood. The process repeats until we generate the full sentence. At this point, we repeat our conditional probability computation one last time and pick the winning sentence. \n",
    "\n",
    "The picture below provides a nice overview of the beam search algorithm.\n",
    "\n",
    "<img src=\"beamsearch.png\" alt=\"Beam Search Overview\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center>\n",
    "<br><br>\n",
    "** Figure 4: Overview of Beam Search **\n",
    "\n",
    "</center>\n",
    "Source: https://github.com/mfederico and https://www.quora.com/Why-is-beam-search-required-in-sequence-to-sequence-transduction-using-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS based Noisy Channel model\n",
    "\n",
    "The POS-based noisy channel model is provided as an augmentation to the beam search algorithm and is applied during the last phase of beam search when all possible full sentence outputs are known. A POS-validation is performed on each of the sentences in rank order of conditional likelihood. The first sentence that passes POS validation is selected as the final output sentence.\n",
    "\n",
    "This is shown in the diagram below\n",
    "\n",
    "<img src=\"noisy-pos.png\" alt=\"Parts of Speech Validation on Beam Search Results\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "** Figure 5: Parts of Speech Vaidation on Beam Search output **\n",
    "\n",
    "</center>\n",
    "\n",
    "The POS validation model is a binary classifier that is trained on valid and invalid parts of speech tags using negative sampling techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation metrics - ROUGE score\n",
    "\n",
    "The ROUGE score is a standad method for evaluation of text summarization. We will use this same metric in order to validate out text summarization as well. \n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation [7]. It comapres an automatically produced summary against a set of reference summaries. In the context of the ROUGE score, a higher recall corresponds to recalling more words from the reference summaries. Precision looks at the number of overlapping words relative to the total words output by the system. Thus precision decreases as the number of spurious words in the system generated output summary decreases.\n",
    "\n",
    "The ROUGE score is compared on n-grams - typically unigrams and bigrams. \n",
    "\n",
    "Another version of the ROUGE score, called ROUGE-S measures the longest matching sequence of words. \n",
    "\n",
    "We will be using one of several available packages for evaluating the ROUGE score of our output summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "This section captures various implementation details of this paper.\n",
    "\n",
    "There are several similarities between this paper and other existing state of the art papers on text summarization.\n",
    "\n",
    "1. Use of sequence to sequence models.\n",
    "2. Use of attention-based  models\n",
    "3. Use of pointer-generation networks\n",
    "4. Use of beam search to select the best output sequence\n",
    "5. Use of CNN/Daily Mail dataset. \n",
    "6. Use of pre-trained Glove embeddings to encode input sentences.\n",
    "\n",
    "The primary areas where this paper differs from the above papers are listed below\n",
    "\n",
    "1. Use of a POS-based validator in the final phase of Beam search. \n",
    "2. Hyperparameter tuning to ensure best results. \n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "In this section, we outline the various steps in the construction of this model. \n",
    "\n",
    "The steps involved in the model are as follows\n",
    "\n",
    "1. Input text is first converted into word embeddings using pretrained Glove embeddings.\n",
    "2. This is fed into a decoder, a bi-directional LSTM network.\n",
    "3. The output of the encoder is fed into the decoder LSTM network, a unidirectional LSTM. \n",
    "4. Attention weights are computed from the hidden weights and output states for each timestep.\n",
    "5. A pointer generation model weights are computed to determine whether the next word should be selected from the decoder or directly from the output text.  \n",
    "6. Decoder output goes through a beam search at each timestep to get a set of k possible best matches, where k is the  beam size.\n",
    "7. In the final step of the decoder, a Parts of Speech (POS) classifer is used to select the first highest rank beam search output that passes POS validation.\n",
    "\n",
    "These steps are captured in the block diagram below.\n",
    "\n",
    "<img src=\"final-blockdiagram.png\" alt=\"End to End Block Diagram\" style=\"width: 800px;\"/>\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "** Figure 6: End to End Block Diagram of Text Summarizer **\n",
    "\n",
    "</center>\n",
    "\n",
    "### Preliminary model parameters \n",
    "\n",
    "The following are initial parameters to the model. Final hyperparameters will be determined following hyper-parameters which takes place during the actual experimentation phase.\n",
    "\n",
    "1. Encoder - Bidirectional LSTM\n",
    "2. Decoder - Unidirectional LSTM\n",
    "3. Hidden Dimension - 500\n",
    "4. Embedding Dimension - 500\n",
    "5. Beam size for beam search - 3\n",
    "6. Hardware/Software resources - Google Cloud Platform.\n",
    "\n",
    "## Experimentation\n",
    "\n",
    "This section captures the experimentation and data collection steps.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset for this model is the CNN/Dailymail dataset[6]. The reason we choose this dataset over other datasets is because this has larger abstracts, whereas most other datasets have single sentence summaries (or headlines). We are interested in generating summaries that span multiple sentences (recall our earlier goal of generating multiline text summaries).\n",
    "\n",
    "The data is split into 70:30 for **training** and **validation**. Training involves k-fold cross-verification to allow for hyperparameter tuning. \n",
    "\n",
    "### Training\n",
    "\n",
    "TBD\n",
    "\n",
    "### Validation\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Anatomy of a Large-Scale Hypertextual Web Search Engine, http://infolab.stanford.edu/~backrub/google.html\n",
    "2. Get to the Point: Summarization with Pointer-Generator Networks, https://arxiv.org/abs/1704.04368\n",
    "3. Neural Machine Translation by Jointly Learning to Align and Translate, https://arxiv.org/abs/1409.0473\n",
    "4. Sequence to Sequence Learning with Neural Networks, https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "5. Modeling with Attention, Lecture 8, Chris Dyer, CMU, https://www.youtube.com/watch?v=ah7_mfl7LD0&t=241s\n",
    "6. CNN/Dailymail dataset, https://cs.nyu.edu/~kcho/DMQA/\n",
    "7. What is ROUGE and How it works for Evaluation of Summarization Tasks?, http://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.WrmhF9PwZTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
